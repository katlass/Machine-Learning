#Preprocessing Data
#Switching categorical variables to binary 
Import pandas as pd
df=pd.read_csv(‘auto.csv’)
df_origen=pd.get_dummies(df)
#to avoid dummy variable trap
df_origen=df_origen.drop(‘origen_Asia, axis=1)
print(df_origen.head())

#-----------------------------------------------------------------------------------------------------------
# Import pandas
import pandas as pd

# Read 'gapminder.csv' into a DataFrame: df
df = pd.read_csv('gapminder.csv')

# Create a boxplot of life expectancy per region
df.boxplot('life', 'Region', rot=60)

# Show the plot
plt.show()

#-----------------------------------------------------------------------------------------------------------
# Create dummy variables: df_region
df_region = pd.get_dummies(df)

# Print the columns of df_region
print(df_region.columns)

# Create dummy variables with drop_first=True: df_region
df_region = pd.get_dummies(df,drop_first=True)

# Print the new columns of df_region
print(df_region.columns)


#-----------------------------------------------------------------------------------------------------------
# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score


# Instantiate a ridge regressor: ridge
ridge = Ridge(alpha=0.5,normalize=True)

# Perform 5-fold cross-validation: ridge_cv
ridge_cv = cross_val_score(ridge,X,y,cv=5)

# Print the cross-validated scores
print(ridge_cv)

#-----------------------------------------------------------------------------------------------------------
#Handling missing data
#This will telling if there are “null rows”
df.info()
#BUT missing values can be encoded in a number of ways like by zeros, or ?s,-1s
#So, you could drop all the missing values (say if they are zeros)
df.insulin.replace(0,np.nan,inplace=True)
df.triceps.replace(0,np.nan,inplace=True)
df.bmi.replace(0,np.nan,inplace=True)
df=df.dropna()
#BUT, look at the shape now ;(
df.shape
>>>(393,9)
#we lost half the rows
#Let’s instead IMPUTE missing data, which means make an educated guess about the missing value
#ex. Use the mean of the non missing values
from sklearn.preprocessing import Imputer
imp=Imputer(missing_values=’Nan’,strategy=’mean’,axis=0)
#axis=0 means we will impute along columns
#axis=1 means we will impute along rows
imp.fit(X)
X=imp.transform(X)
#OR we could fit our model and impute at the same time using scikit-learn pipeline object
From sklearn.pipeline import Pipeline
from sklearn.preprocessing import Imputer
imp=Imputer(missing_values=’Nan’,strategy=’mean’,axis=0)
model=LogisticRegression()
#now to build the pipeline object, we construct a list of steps in the pipeline, where each step is a two tuple with the ame you want to give the step and the actual step(estimator)
Steps=[(‘Imputation’,imp),(‘Logistic_Regression’,model)]
#Then pass this list to the pipeline constructor
pipeline=Pipeline(Steps)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.4,random_state=42)
pipeline.fit(X_train,y_train)
Yhat=pipeline.predict(X_test)
#compute accuracy
pipeline.score(X_test,y_test)
#In a pipeline, each step but the last must be a transformer, and the last must be an estimator such as a classifier, regressor or transformer 

#-----------------------------------------------------------------------------------------------------------
# Convert '?' to NaN
df[df == '?'] = np.nan

# Print the number of NaNs
print(df.isnull().sum())

# Print shape of original DataFrame
print("Shape of Original DataFrame: {}".format(df.shape))

# Drop missing values and print shape of new DataFrame
df = df.dropna()

# Print shape of new DataFrame
print("Shape of DataFrame After Dropping All Rows with Missing Values: {}".format(df.shape))


#-----------------------------------------------------------------------------------------------------------
# Import the Imputer module
from sklearn.preprocessing import Imputer
#Look into Support Vector Classification, a type of SVM
from sklearn.svm import SVC


# Setup the Imputation transformer: imp
imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)

# Instantiate the SVC classifier: clf
clf = SVC()

# Setup the pipeline with the required steps: steps
steps = [('imputation', imp),
        ('SVM', clf)]

#-----------------------------------------------------------------------------------------------------------

# Import necessary modules
from sklearn.preprocessing import Imputer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

# Setup the pipeline steps: steps
steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),
        ('SVM', SVC())]

# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)

# Fit the pipeline to the train set
pipeline.fit(X_train,y_train)

# Predict the labels of the test set
y_pred = pipeline.predict(X_test)

# Compute metrics
print(classification_report(y_test, y_pred))
#-----------------------------------------------------------------------------------------------------------
#Centering and Scaling
print(df.describe())
#this gives you info about range, mean, std, count

#ways to normalize, standardize data

from sklearn.preprocessing import scale
X_scaled=scale(X)
#but what kind of scaling is this?????

From sklearn.preprocessing import StandardScalar
steps=[(‘Scale’,StandardScalar()),(‘knn’,KNeighborsClassifier())]
pipeline=Pipeline(steps)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)
knn_scaled=pipeline.fit(X_train,y_train)
Yhat=pipeline.predict(X_test)
accuracy_score(y_test,Yhat)
#if we didn’t perform scaling, the accuracy score was lower!
#-----------------------------------------------------------------------------------------------------------
#Using cross validation with a supervised learning pipeline
steps=[(‘Scale’,StandardScalar()),(‘knn’,KNeighborsClassifier())]
pipeline=Pipeline(steps)
#now specify the hyperparameter space by creating a dictionary, the keys are pipeline step name followed by a double underscore followed by the hyperparameter name, value is a list or array of values to try for that hyperparameter
parameters={‘knn__n_neighbors’=np.arange(1,50)}
#why is it an = not : ????
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)
cv=GridSearchCV(pipeline,param_grid=parameters)
cv.fit(X_train,y_train)
yHat=cv.predict(X_test)
print(cv.best_params_)
print(cv.score(X_test,y_test))
print(classification_report(y_test,yHat)

#-----------------------------------------------------------------------------------------------------------
# Import scale
from sklearn.preprocessing import scale

# Scale the features: X_scaled
X_scaled = scale(X)

# Print the mean and standard deviation of the unscaled features
print("Mean of Unscaled Features: {}".format(np.mean(X))) 
print("Standard Deviation of Unscaled Features: {}".format(np.std(X)))

# Print the mean and standard deviation of the scaled features
print("Mean of Scaled Features: {}".format(np.mean(X_scaled))) 
print("Standard Deviation of Scaled Features: {}".format(np.std(X_scaled)))
#-----------------------------------------------------------------------------------------------------------

# Import the necessary modules
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


# Setup the pipeline steps: steps
steps = [('scaler', StandardScaler()),
        ('knn', KNeighborsClassifier())]
        
# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)

# Fit the pipeline to the training set: knn_scaled
knn_scaled = pipeline.fit(X_train,y_train)

# Instantiate and fit a k-NN classifier to the unscaled data
knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)

# Compute and print metrics
print('Accuracy with Scaling: {}'.format(pipeline.score(X_test,y_test)))
print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test,y_test)))
#-----------------------------------------------------------------------------------------------------------
#Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality.You'll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: Do not worry about this now as it is beyond the scope of this course. (look into kernel coefficient) 
# Setup the pipeline
steps = [('scaler', StandardScaler()),
         ('SVM', SVC())]

pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {'SVM__C':[1, 10, 100],
              'SVM__gamma':[0.1, 0.01]}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=21)

# Instantiate the GridSearchCV object: cv
cv = GridSearchCV(pipeline,param_grid=parameters)

# Fit to the training set
cv.fit(X_train,y_train)

# Predict the labels of the test set: y_pred
y_pred = cv.predict(X_test)

# Compute and print metrics
print("Accuracy: {}".format(cv.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Parameters: {}".format(cv.best_params_))
#-----------------------------------------------------------------------------------------------------------


# Setup the pipeline steps: steps
steps = [('imp', Imputer(missing_values='NaN', strategy='mean', axis=0)),
         ('scaler',StandardScaler()),
         ('elasticnet', ElasticNet())]

# Create the pipeline: pipeline 
pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)

# Create the GridSearchCV object: gm_cv
gm_cv = GridSearchCV(pipeline,param_grid=parameters)

# Fit to the training set
gm_cv.fit(X_train,y_train)

# Compute and print the metrics
r2 = gm_cv.score(X_test, y_test)
print("Tuned ElasticNet Alpha: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet R squared: {}".format(r2))
#-----------------------------------------------------------------------------------------------------------




