{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df12097-907d-4ea8-9812-524c12b8ee8d",
   "metadata": {},
   "source": [
    "# Generalized Linear Models in Python\n",
    "\n",
    "Can handle data where the response variable is either binary, count, or approximately normal, all under one single framework: The Generalized Linear Models. Extend your regression toolbox with the logistic and Poisson models, by learning how to fit, understand, assess model performance and finally use the model to make predictions on new data. You will practice using data from real world studies such the largest population poisoning in world's history, nesting of horseshoe crabs and counting the bike crossings on the bridges in New York City"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2590bb-951a-4ff4-a675-dfa03768e37a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "-WHY: GLMs provide a versatile framework for statistical modeling of data and are often used to solve practical problems. \n",
    "-WHAT: GLMs are a generalization of linear models. To understand this suppose you would like to predict salary given years of experience. In regression terms you would write it as salary is predicted by experience where tilde means \"predicted by\" (salary ~ experience). More formally, our linear model would be written as follows: y= B0+B1x1+E\n",
    "ols(formula = 'y ~ X', data=my_data).fit() vs glm(formula = 'y ~ X', data=my_data, family = sm.families___).fit()\n",
    "-DIFFERENCE: for glm one additional argument, family, which denotes the probability distribution of the response variable.\n",
    "-OLS: The regression function tells us how much the response variable y changes, on average, for a unit increase in x. \n",
    "    -The model assumptions are linearity in the parameters, the errors are independent, normally distributed and the variance around the regression line is constant for all values of x.\n",
    "        -The response variable is continuous and that a Gaussian distribution for the response can be applied for the model formulation. In case of a binary response variable, the response distribution is Binomial providing for the estimated porbabilities to be bounded by 0,1.\n",
    "-GLM: If the response is not continuous but binary or count (think binomial/poisson distribution), or the variance of y is not constant - variance depends on the mean.   \n",
    "-OLS FOR BINARY DATA: will get values above 1 which don't make sense. To correct for this we fit a GLM, with the Binomial family corresponding to Binomial or logistic regression. Visually, there is a significant difference in the fitted models, probability is bounded by 0 and 1.\n",
    "-CLASS DETERMINATION: To obtain the binary class from computed probabilities, we split the probabilities at say 0.5, which gives one of either class.\n",
    "-WHY GLM: linear models are not suitable to accommodate different than continuous response data and provide rather strange results. GLMs provide a unified framework for modeling data originating from the exponential family of densities which include Gaussian, Binomial, and Poisson, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a48b5-13c2-478e-9873-cd8458cf2779",
   "metadata": {},
   "source": [
    "Exercise 1: Linear model, a special case of GLM\n",
    "In this exercise you will fit a linear model two ways, one using the ols() function and one using the glm() function. This will show how a linear model is a special case of a generalized linear model (GLM).\n",
    "You will use the preloaded salary dataset introduced in the video.\n",
    "Recall that the linear model in Python is defined as:\n",
    "ols(formula = 'y ~ X', data = my_data).fit()\n",
    "and the generalized linear model can be trained using\n",
    "glm(formula = 'y ~ X', data = my_data, family = sm.families.___).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc5907-84c1-4c8f-a67b-234419b9a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols,glm\n",
    "\n",
    "# Fit a linear model\n",
    "model_lm = ols(formula = 'Salary ~ Experience',\n",
    "               data = salary).fit()\n",
    "\n",
    "# View model coefficients\n",
    "print(model_lm.params)\n",
    "\n",
    "\n",
    "from statsmodels.formula.api import ols, glm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit a GLM\n",
    "model_glm = glm(formula = 'Salary ~ Experience',\n",
    "                data = salary,\n",
    "                family = sm.families.Gaussian()).fit()\n",
    "\n",
    "# View model coefficients\n",
    "print(model_glm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14151e-b427-4c46-9c27-d1a4a2b40d73",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "OLS and GLM results are the same when family set to Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b30c5-21bb-4812-b996-1e8f01f58ece",
   "metadata": {},
   "source": [
    "## Components of the GLM\n",
    "g(E[y])= B0 +  B1x1 + ... + BpXp\n",
    "There are three components of GLM:\n",
    "1. The random component which defines the response variable y and its probability distribution. As we saw previously there are different response data types to consider depending on your data problem. One important assumption here is that the observations y_1 through y_n are independent. (y)\n",
    "2. The systematic component which defines which explanatory variables to include in the model. We can include p different explanatory variables. (x1...xp)\n",
    "    -Note that it allows for interaction effects, where one variable depends on another and vice versa\n",
    "    -Or curvilinear effects (x1^2), etc. \n",
    "    -Note that the RHS represents a linear combination of the explanatory variables.\n",
    "3. The link function, which connects the random and systematic component (g()). \n",
    "    -It is the function of the expected value of the response variable which enables linearity in the parameters. \n",
    "    -By its construction it allows the mean of the response variable to be nonlinearly related to the explanatory variables. \n",
    "    -It is the link function that generalizes the linear model. \n",
    "    -The link function transforms the expected value of y and not y itself, and it enables linear combinations\n",
    "    -Note that the choice of the link function is separate from the choice of random component. \n",
    "\n",
    "## Most common data types and how they are represented in the GLM framework.\n",
    "-Continuous Linear Regression, approximately normally distributed with domain (-inf,inf). Some of the examples are house prices, level of salary, person's height, etc. \n",
    "    -When fitting a GLM we would use Gaussian for the distribution family where the link function is the identity: g(mu) = mu = E(y) . The identity link function is of the simplest form where it equals mu or the mean of the response. \n",
    "    -Linear regression is a special case of the GLM.\n",
    "-Binary Logistic Regression\n",
    "    -To fit a GLM you should use Binomial distribution where the default link function is the logit: g(mu)=log(p/(1-p))\n",
    "-Count Poisson regression\n",
    "    -Count data are positive [0,inf] and some examples include the number of hurricanes, number of bike crossing on a bridge, etc. \n",
    "    -To fit a GLM you should use Poisson for the distribution where the default link function is logarithm: g(mu)=log(mu)\n",
    "    -Ex: You decide to fit a GLM model, the number of bike crossings, poisson distribution family used for fitting a GLM model\n",
    "-Others\n",
    "    -Gamma: g(mu) = 1/mu\n",
    "    -Inverse Gaussian: g(mu) = 1/mu^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd991cb-4cf1-4a31-be3d-3b4eba22f458",
   "metadata": {},
   "source": [
    "Exercise 2: Linear model and a binary response variable\n",
    "In the video, you saw an example of fitting a linear model to a binary response variable and how things can go wrong quickly. You learned that, given the linear line fit, you can obtain fitted values \n",
    ", which are not in line with the logic of the problem since the response variable takes on values 0 and 1.\n",
    "Using the preloaded crab dataset, you will study this effect by modeling y as a function of x using the GLM framework.\n",
    "Recall that the GLM model formulation is:\n",
    "glm(formula = 'y ~ X', data = my_data, family = sm.families.____).fit()\n",
    "where you specify formula, data, and family.\n",
    "Also, recall that a GLM with:\n",
    "the Gaussian family is a linear model (a special case of GLMs)\n",
    "the Binomial family is a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea086588-cc94-4a68-b59d-bd37e374ac92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1f/dhdnpvy122jbjc2tp2wd_nq80000gn/T/ipykernel_46332/3046608107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define probability distribution for the response variable for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the linear (LM) and logistic (GLM) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfamily_LM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamilies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfamily_GLM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamilies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "# Define model formula\n",
    "formula = 'y ~ width'\n",
    "\n",
    "# Define probability distribution for the response variable for \n",
    "# the linear (LM) and logistic (GLM) model\n",
    "import statsmodels as sm\n",
    "family_LM = sm.families.Gaussian()\n",
    "family_GLM = sm.families.Binomial()\n",
    "\n",
    "# Define and fit a linear regression model\n",
    "model_LM = glm(formula = formula, data = crab, family = family_LM).fit()\n",
    "print(model_LM.summary())\n",
    "\n",
    "# Define and fit a logistic regression model\n",
    "model_GLM = glm(formula = formula, data = crab, family = family_GLM).fit()\n",
    "print(model_GLM.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e95a2-e123-4d9b-a229-8870bf81e062",
   "metadata": {},
   "source": [
    "Exercise 3: Comparing predicted values\n",
    "In the previous exercise, you have fitted both a linear and a GLM (logistic) regression model using crab data, predicting ywith width. In other words, you wanted to predict the probability that the female has a satellite crab nearby given her width.\n",
    "In this exercise, you will further examine the estimated probabilities (the output) from the two models and try to deduce if the linear fit would be suitable for the problem at hand.\n",
    "The usual practice is to test the model on new, unseen, data. Such dataset is called test sample.\n",
    "The test sample has been created for you and loaded in the workspace. Note that you need test values for all variables present in the model, which in this example is width.\n",
    "The crab dataset has been preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3725809-d7de-4544-9436-9651ea5d2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test set\n",
    "print(test)\n",
    "\n",
    "# Compute estimated probabilities for linear model: pred_lm\n",
    "pred_lm = model_LM.predict(test)\n",
    "\n",
    "# Compute estimated probabilities for GLM model: pred_glm\n",
    "pred_glm = model_GLM.predict(test)\n",
    "\n",
    "# Create dataframe of predictions for linear and GLM model: predictions\n",
    "predictions = pd.DataFrame({'Pred_LM': pred_lm, 'Pred_GLM': pred_glm})\n",
    "\n",
    "# Concatenate test sample and predictions and view the results\n",
    "all_data = pd.concat([test, predictions], axis = 1)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17294a41-93f2-4b67-af63-2f121c6e20e3",
   "metadata": {},
   "source": [
    "## Fit a GLM in Python using statsmodels\n",
    "-Note: Each explanatory variable is specified and separated with a plus sign. \n",
    "-Note that the formula needs to be enclosed in quotation marks. \n",
    "-There are different ways we can represent explanatory variables in the model. \n",
    "    -Categorical variables are enclosed with capital C\n",
    "    -removing the intercept is done with minus one\n",
    "    -the interaction terms are written in two ways depending on the need, where the semicolon applies to only the interaction term, whereas the multiplication symbol will also, in addition to the interaction term, add individual variables. \n",
    "    -Can also add transformations of the variables directly in the formula '... + np.log(x) +...'\n",
    "    \n",
    "Family Argument\n",
    "-Family distributions are in the families namespace: families = sm.families.four underscores().\n",
    "-The default link function is denoted in parenthesis, but you could choose other link functions available for each distribution. However, if you choose to use a non-default link function, you would have to specify it directly.\n",
    "sm.families.Gaussian(link = sm.families.links.indentity)\n",
    "sm.families.Binomial(link = sm.families.links.logit) or .probit, .cauchy, .log, .cloglog\n",
    "sm.families.Poisson(link = sm.families.links.log)  or identity, sqrt\n",
    "\n",
    "Model Outputs:\n",
    "-.summary() provides the main information on model fit, such as the model description, model statistics such as log-likelihood and deviance, and estimated model parameters with their corresponding statistics. The estimated parameters are given by coef with their standard error, z scores, p-values and 95% confidence intervals.\n",
    "-.params to only view the regression coefficients given model fit. \n",
    "-.conf_int() the confidence intervals for the parameter estimates. The default is 95% which you can change using the alpha argument. With cols argument, you can specify which confidence intervals to return.\n",
    "-.predict() to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76e9a0-061c-40a4-85aa-036dba832e0e",
   "metadata": {},
   "source": [
    "Exercise 4: Model fitting step-by-step\n",
    "you learned the key components for fitting a GLM in Python using the statsmodels package. In this exercise you will define the components of the GLM step by step and finally fit the model by calling the .fit() method.\n",
    "The dataset which you will use is on the contamination of groundwater with arsenic in Bangladesh where we want to model the household decision on switching the current well.\n",
    "The columns in the dataset are:\n",
    "switch: 1 if the change of the current well occurred; 0 otherwise\n",
    "arsenic: The level of arsenic contamination in the well\n",
    "distance: Distance to the closest known safe well\n",
    "education: Years of education of the head of the household\n",
    "Dataset wells has been preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c7ff4-2f25-433b-8ccc-8a0a4f3a0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formula the the logistic model\n",
    "model_formula = 'switch ~ distance100'\n",
    "\n",
    "# Define the correct probability distribution and the link function of the response variable\n",
    "link_function = sm.families.links.logit\n",
    "model_family = sm.families.Binomial(link = link_function)\n",
    "\n",
    "# Fit the model\n",
    "wells_fit = glm(formula = model_formula , \n",
    "                 data = wells, \n",
    "                 family = model_family).fit()\n",
    "# View the results of the wells_fit model\n",
    "print(wells_fit.summary())\n",
    "\n",
    "# Extract coefficients from the fitted model wells_fit\n",
    "intercept, slope = wells_fit.params\n",
    "\n",
    "# Print coefficients\n",
    "print('Intercept =', intercept)\n",
    "print('Slope =', slope)\n",
    "\n",
    "# Extract and print confidence intervals\n",
    "print(wells_fit.conf_int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88487ae4-6545-40ce-bd6e-0aa0147e06de",
   "metadata": {},
   "source": [
    "## Binomial Data\n",
    "-A single event, like the flip of a coin, with two outcomes has a Bernoulli distribution with probability p. \n",
    "    -Note: Bernoulli deals with the outcome of the single trial of the event, whereas Binomial deals with the outcome of the multiple trials of the single event. This is a special case of Binomial with n equal to 1.\n",
    "-logistic function = f(z) = 1/(1+exp(-z))\n",
    "-Odds and odds ratio\n",
    "    -Odds = event occurring/event not occurring\n",
    "    -Odds: odds1/odds2\n",
    "    -Example: given 4 games(outcome: W,W,W,L), the odds of winning a game are 3 to 1, meaning that the event win occurred 3 times and loss once.\n",
    "-Odds and probabilities\n",
    "    -Odds are not probabilities but are directly related and can be computed from each other. \n",
    "    -Example: \n",
    "        -Odds = probability of the event to the probability of non-event = probability/(1-probability)\n",
    "        -Writing probability in terms of odds helps us transform our initial unbounded probability model by removing the upper bound whereas the logarithm of odds removes the lower bound.\n",
    "            -probability=odds/1-odds\n",
    "-Steps: from probability model to logistic regression\n",
    "    -First, recall our initial model from chapter 1, which we couldn't fit with linear unbounded function. \n",
    "    -Applying the logistic function to the model provides the necessary bounds required for our binary data. \n",
    "    -We compute mu, the estimated probability but also the probability of the event not occurring, i.e. 1 minus mu.\n",
    "    -Finally, computing the odds in terms of probability with log transformation is central to logistic regression. It provides for many desirable properties as in linear regression, which we will see later on. \n",
    "    -Note that the logit is linear in its parameters, can range from minus infinity to infinity depending on the range of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b29de5-28f3-463e-83af-48b22d573a1b",
   "metadata": {},
   "source": [
    "Exercise 5: Compute odds and probabilities\n",
    "In this exercise you will review the concept of odds and their relationship to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e656f4f-e652-41d3-b8ab-56627ce51e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#An athlete competes in 60 races and wins 15 times. Compute and print the odds of winning.\n",
    "# Compute the odds\n",
    "odds = 15/(60-15)\n",
    "\n",
    "# Print the result\n",
    "print('Odds are: ', round(odds,3))\n",
    "#Odds are:  0.333\n",
    "#####################################################################################################################\n",
    "#An athlete competes in 60 races and wins 15 times. Compute and print the probability of winning.\n",
    "#Compute the probability\n",
    "probability = 15/60\n",
    "\n",
    "# Print the result\n",
    "print('Probability is: ', round(probability,3))\n",
    "#Probability is:  0.25\n",
    "#####################################################################################################################\n",
    "#Compute and print the odds using the computed probabilities from previous exercise\n",
    "# Probability calculation\n",
    "probability = 15/60\n",
    "\n",
    "# Compute odds using probability calculation\n",
    "odds_from_probs = probability/(1 - probability)\n",
    "\n",
    "# Print the results\n",
    "print(round(odds_from_probs, 3))\n",
    "#Odds: 0.333"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76d899-6cf5-40f8-b940-818c943b0019",
   "metadata": {},
   "source": [
    "Exercise 6: Fit logistic regression\n",
    "In this exercise, you will continue with the data from the study on the contamination of ground water with arsenic in Bangladesh where you want to model the probability of switching the current well given the level of arsenic present in the well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8492c-f565-431c-966d-559c48b8bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and functions\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit logistic regression model\n",
    "model_GLM = glm(formula = 'switch ~ arsenic',\n",
    "                data = wells,\n",
    "                family = sm.families.Binomial()).fit() \n",
    "\n",
    "# Print model summary\n",
    "print(model_GLM.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafa82d-222f-47b8-aab8-1c95c2ae75cc",
   "metadata": {},
   "source": [
    "## Model coefficients\n",
    "-Coefficient beta: determines the rate of increase or decrease of the sigmoid curve. \n",
    "    -a positive coefficient with an ascending sigmoid curve. \n",
    "    -negative coefficient with a descending sigmoid curve.\n",
    "-Challenging to interpret coefficients of logistic regression due to nonlinearity. \n",
    "    -Interpretation of the coefficients for the logistic regression is the same as for linear models except that in logistic regression coefficients are in terms of the log odds. \n",
    "    -Example:\n",
    "        -In linear model, for every one-unit increase in weight, the estimated probability increases by 0.32. \n",
    "        -In the logit model for every one-unit increase in weight the log odds increase by 1.8. Clearly, the two interpretations are not the same. But what does an increase in log odds actually mean?\n",
    "Log odds interpretation: \n",
    "-Starting from the logistic model: \n",
    "log(mu/1-mu) = B0 + B1x1 (intepretation: one correct for how many incorrect = odds) (odds: wins to losses, probability: is wins to overall number of observations) \n",
    "\n",
    "-assume a one-unit increase in x. Expanding the parenthesis we obtain the following expression. \n",
    "log(mu/1-mu) = B0 + B1(x1+1) = B0 + B1x1 + B1\n",
    "\n",
    "-To obtain the odds we take the exponential and rearrange the terms. \n",
    "mu/1-mu = exp(B0 + B1x1)exp(B1)\n",
    "\n",
    "-We see that the odds (exp(B0 + B1x1)) are multiplied by the exponential of the coefficient(exp(B1)). \n",
    "-Example:  log(mu/1-mu) = -3.6947 + 1.815 * weight\n",
    "    -Given the weight coefficient of 1.815, the odds of satellite crab multiply by 6.14 (exp(1.815)) for every unit increase in weight.\n",
    "    -The intercept value of -3.6947 provides the baseline log odds, by assuming zero values for the weight variable. \n",
    "    \n",
    "Sometimes it is more natural to think in terms of probabilities than log odds.\n",
    "\n",
    "## Probability vs logistic fit\n",
    "-Example: how the estimated probability changes as hours of study change? The curve is nonlinear so the rate of change in probability per 1-unit increase in hours of study will depend on the value of hours.\n",
    "    -Starting from hours 1, 2 or 6,7 and increasing it by one unit does not change the estimated probability by much. \n",
    "    -However, starting from 4, 4.5 increases the estimated probability significantly. \n",
    "    -To compute this rate of change at a particular value of x, we compute the slope of the tangent line at the value of x. \n",
    "    -For the coefficient beta, slope = beta*probability(1-probability)\n",
    "    -Generally, the steepest slope occurs at the point where the probability mu equals 0.5.\n",
    "\n",
    "-Compute change in estimated probability\n",
    "    -Considering the horseshoe crab model, we compute the rate of change in the estimated probability given a one-unit change in weight. \n",
    "    -First, we choose the value of weight (x1) for which to make the computation and extract model coefficients. \n",
    "    -Then using the probability formula from our logit model we compute the estimated probability. \n",
    "        mu/1-mu (estimated probability) = exp(B0 + B1*x)/(1+exp(B0 + B1*x)\n",
    "    -Finally, we compute the incremental rate of change of the estimated probability.\n",
    "        -incremental change in etestimated probability given x = slope*estimated probability*(1-estimated probability)\n",
    "         -Ex: Adding 1kg in weight when weight is 1.5 corresponds to a positive difference in the probability of satellite crab of 36%.\n",
    "-Rate of change in probability for every x\n",
    "    -Continuing computation for other values of x we obtain a curve, from which we can see that the biggest increase in the estimated probability is around the median value of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36fda4-c3cf-4041-a829-221a38e071d6",
   "metadata": {},
   "source": [
    "Exercise 7: Coefficients in terms of odds\n",
    "Previously you have fitted a logistic regression model for the probability of switching the well given the arsenic levels. In this exercise, you will see how another variable distance100 relates to the probability of switching and interpreting the coefficient values in terms of odds.\n",
    "Recall that the logistic regression model is in terms of log odds, so to obtain by how much would the odds multiply given a unit increase in x you would exponentiate the coefficient estimates. This is also called odds ratio.\n",
    "Recall that odds are a ratio of event occurring to the event not occurring. For example, if the odds of winning a game are 1/2 or 1 to 2 (1:2), it means that for every one win there are 2 losses.\n",
    "The dataset wells is loaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc894e74-415b-41be-93d4-6f25e3b4a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and functions\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "import numpy as np\n",
    "\n",
    "# Fit logistic regression model\n",
    "model_GLM = glm(formula = 'switch ~ distance100',\n",
    "                data = wells,\n",
    "                family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Extract model coefficients\n",
    "print('Model coefficients: \\n', model_GLM.params)\n",
    "\n",
    "# Compute the multiplicative effect on the odds\n",
    "print('Odds: \\n', np.exp(model_GLM.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b3cc1-3ace-4674-86cd-d3d18972532a",
   "metadata": {},
   "source": [
    "Output:\n",
    "    -Model coefficients: \n",
    "     Intercept      0.605959\n",
    "    distance100   -0.621882\n",
    "    dtype: float64\n",
    "    -Odds: \n",
    "     Intercept      1.833010\n",
    "    distance100    0.536933\n",
    "    dtype: float64\n",
    "\n",
    "Interpretation:\n",
    "The odds of switching the well is 1/2 for a 1-unit (100m) increase in distance, so for every one switch (household switches to the nearest safe well) there would be 2 households who would not switch to the nearest safe well.\n",
    "With one-unit increase in distance100 the log odds increase by -0.621882"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47ab3c-82b1-480e-ac38-caa628a26a08",
   "metadata": {},
   "source": [
    "Exercise 8: Rate of change in probability\n",
    "For the wells dataset you have already fitted a logistic regression model with the model formula switch ~ distance100 obtaining the following fit\n",
    "In this exercise you will use that model to understand how the estimated probability changes at a certain value of distance100, say 1.5 as depicted in the figure below.\n",
    "Recall the formulas for the inverse-logit (probability)\n",
    "and the slope of the tangent line of the model fit at point :\n",
    "Dataset wells and the model wells_GLM are loaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b2e03-f71c-4fbf-b98f-ae99c8fa327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x at 1.5\n",
    "x = 1.5\n",
    "# Extract intercept & slope from the fitted model\n",
    "intercept, slope = wells_GLM.params\n",
    "\n",
    "# Compute and print the estimated probability\n",
    "est_prob = np.exp(intercept + slope*x)/(1+np.exp(intercept + slope*x))\n",
    "print('Estimated probability at x = 1.5: ', round(est_prob, 4))\n",
    "\n",
    "# Compute the slope of the tangent line for parameter beta at x\n",
    "slope_tan = slope * est_prob * (1 - est_prob)\n",
    "print('The rate of change in probability: ', round(slope_tan,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac20ce-4bf4-4462-9249-0ad4ebd44d18",
   "metadata": {},
   "source": [
    "Outcome:\n",
    "Estimated probability at x = 1.5:  0.419\n",
    "The rate of change in probability:  -0.1514\n",
    "\n",
    "Interpretation:\n",
    "So at the distance100 value of 1.5 the estimated probability is 0.419 with the rate of change in the estimated probability of negative 0.1514. This means that for every 1oo m increase in distance100 at the distance100 value of 1.5 the probability of well switch decreases by 15.14%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e10675-2da8-4808-ae69-3f3ba0d29fd1",
   "metadata": {},
   "source": [
    "## Estimation of beta coefficient\n",
    "-The regression coefficients are obtained by the maximum likelihood estimation, where the value of the parameters maximizes the probability of the observed data. \n",
    "    -Recall that the likelihood is the probability of data given coefficient estimates and that maximizing likelihood or loglikelihood is mathematically equivalent. \n",
    "    -The estimated beta coefficient is where the likelihood takes on its maximum value.\n",
    "    -Unlike least squares estimation in linear regression, the maximization of the likelihood with respect to beta usually requires an iterative solution, which is sometimes called IRLS or iteratively reweighted least squares.\n",
    "\n",
    "\n",
    "## Significance testing:\n",
    "-Use the information provided in the model summary, namely standard error, zvalue, its pvalue, and confidence intervals.\n",
    "-standard error is the standard deviation of a statistic, i.e. coefficient, and its value depends on the shape of the likelihood. \n",
    "    -Imagine the likelihood to be the mountain you are about to ascend. If the top is flatter it is hard to determine the summit, but if it sharp then the summit is easily found with less error. \n",
    "    -Similarly, with likelihood sharper at its peak, the location of the maximum is clearly defined with smaller standard error and vice versa\n",
    "     -Computation of the standard error: \n",
    "         -Take the square root of the variance for the variable. \n",
    "         -The value of the variance we obtain from the diagonal entries of the model variance-covariance matrix, which is obtained using the .cov_params() function. \n",
    "             -Ex: The variance of weight is 0.142. Taking the square root we get 0.37 as in the model summary.\n",
    "-With significance testing we are concerned whether constraining the parameter values to zero would reduce the model fit\n",
    "    -Use z statistic as the ratio of the estimated coefficient and its standard error, which follows the standard normal distribution. : z= Beta/standard error\n",
    "    -For zvalue greater than 2 we say that the variable is statistically significant. \n",
    "-Should always report confidence intervals as they provide information on the uncertainty of the estimates. \n",
    "    -A large sample Wald confidence interval for the coefficient is computed as follows where beta is the estimate and SE its standard error.\n",
    "        -The Wald CI, also called the Wald interval or Classical Large-Sample Interval, is a common method to find binomial confidence intervals.\n",
    "        -The Wald CI doesn’t perform well for small samples or when proportions are close to 0 or 1. \n",
    "        -Wald intervals are based on the normal approximation to the binomial distribution. \n",
    "            -This means that Wald intervals are not suitable for small sample sizes, as their advertised coverage is lower than the actual interval. For example, when successes are not near 50%m its average coverage is around 60%, not 95% \n",
    "            -Wald CI should only be used for larger samples. How large? The performance of the formula relies not only on the sample size, but the unknown population proportion.\n",
    "                -Many textbook state the “magic number” is a sample size > 10, but this should be used with caution. \n",
    "                -Adjusted Wald Confidence intervals may help with small samples.\n",
    "            -Using a Wald CI to test a hypothesis about a population proportion may increase in Type I or Type II errors\n",
    "                -you may be able avoid this possibility by using the hypothesized value, p0, to calculate the standard error instead of the  sample proportion\n",
    "-Using the conf_int() function we can extract the confidence intervals from the fitted model\n",
    "-To interpret confidence intervals in terms of the odds, as we did with coefficients, recall that we used exponentiation to go from log odds to odds. \n",
    "    -Therefore, to obtain confidence intervals for the multiplicative effect on the odds of a unit increase in x we extract the confidence intervals for beta and exponentiate its endpoints. \n",
    "    -Ex: can conclude that a unit increase in weight multiplies the odds by at least 2.93, from the lower bound, and at most by 12.85, from the upper bound, that a satellite is present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe13eb-b5c8-4620-aa87-445884053a1e",
   "metadata": {},
   "source": [
    "Exercise 9: Statistical significance\n",
    "In this exercise you will assess the significance of the estimated coefficients but with width as explanatory variable instead.\n",
    "Recall that coefficients help us determine the significance of the relationship that we are trying to model, where a positive sign increases the probability of an event as the predictor increases and vice versa.\n",
    "The dataset crab is loaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6d54e-8ea1-42ab-b894-a9c6c072b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and th glm function\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit logistic regression and save as crab_GLM\n",
    "crab_GLM = glm('y~ width', data = crab, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(crab_GLM.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598eebeb-074b-44a2-b8a6-1c5ac55b515b",
   "metadata": {},
   "source": [
    "                     Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      171\n",
    "    Model Family:                Binomial   Df Model:                            1\n",
    "    Link Function:                  logit   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -97.226\n",
    "    Date:                Wed, 08 Jun 2022   Deviance:                       194.45\n",
    "    Time:                        14:49:57   Pearson chi2:                     165.\n",
    "    No. Iterations:                     4   Covariance Type:             nonrobust\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept    -12.3508      2.629     -4.698      0.000     -17.503      -7.199\n",
    "    width          0.4972      0.102      4.887      0.000       0.298       0.697\n",
    "    ==============================================================================\n",
    "\n",
    "Outcome:\n",
    "Yes, the estimate is positive meaning that the fit is upward sloping which means that width increases the chance of a satellite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66489c42-cd49-4412-a01f-6d609faadcc6",
   "metadata": {},
   "source": [
    "Exercise 10: Computing Wald statistic\n",
    "In this exercise you will assess the significance of the width variable by computing the Wald statistic.\n",
    "Also note that in the model summary the Wald statistic is presented by the letter z which means that the value of a statistic follows a standard normal distribution. Recall the formula for the Wald statistic.\n",
    "The fitted model crab_GLM and crab dataset have been preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e53283-92bb-4019-973f-ed05fe4b1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "intercept, slope =crab_GLM.params\n",
    "\n",
    "# Estimated covariance matrix: crab_cov\n",
    "crab_cov = crab_GLM.cov_params()\n",
    "print(crab_cov)\n",
    "\n",
    "# Compute standard error (SE): std_error\n",
    "std_error = np.sqrt(crab_cov.loc['width', 'width'])\n",
    "print('SE: ', round(std_error, 4))\n",
    "\n",
    "# Compute Wald statistic\n",
    "wald_stat = slope/std_error\n",
    "print('Wald statistic: ', round(wald_stat,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedda13-696e-4cb6-bf6e-56d91591c085",
   "metadata": {},
   "source": [
    "               Intercept     width\n",
    "    Intercept   6.910158 -0.266848\n",
    "    width      -0.266848  0.010350\n",
    "    SE:  0.1017\n",
    "    Wald statistic:  4.8875\n",
    "    \n",
    "Outcome:\n",
    "With the Wald statistic at 4.887 we can conclude that the width variable is statistically significant if we apply the rule of thumb cut-off value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fda44f-109c-461f-a79c-4135e1154269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuing from the previous exercise you will now asses the uncertainty of the coefficients by computing the confidence intervals.\n",
    "\n",
    "# Extract and print confidence intervals\n",
    "print(crab_GLM.conf_int())\n",
    "\n",
    "# Compute confidence intervals for the odds\n",
    "print(np.exp(crab_GLM.conf_int()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b9eee-aa5c-45f2-aa78-e6f0140b0f39",
   "metadata": {},
   "source": [
    "                    0         1\n",
    "Intercept -17.503010 -7.198625\n",
    "width       0.297833  0.696629\n",
    "\n",
    "\n",
    "                      0         1\n",
    "Intercept  2.503452e-08  0.000748\n",
    "width      1.346936e+00  2.006975\n",
    "\n",
    "\n",
    "Outcome:\n",
    "We can conclude that a 1 cm increase in width of a female crab has at least 35% increase odds (from lower bound) and at most it doubles the odds (from upper bound) that a satellite crab is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37bd92-f546-4648-9354-bbd758f4e35f",
   "metadata": {},
   "source": [
    "## Computing and describing predictions\n",
    "-Logistic regression models usually produce two types of predictions, the probability and the predicted class, 0 or 1. \n",
    "-Finding odds for a single observation:\n",
    "    mu = exp(intercept + slope*x1)/1+exp(intercept + slope*x1)\n",
    "    -Use .predict() to calculate this: outputs a probability\n",
    "    -Now classify them using some specified cutoff value.\n",
    "-Computing class predictions\n",
    "    -extract estimated probabilities using .fittedvalues function and convert into an array using .values. \n",
    "    -Next, define the probability cutoff. \n",
    "    -Prediction class is computed based on the cutoff\n",
    "    -Finally, we count the number of observations in each class and report in the table. \n",
    "        -For cutoff at 0.4, there are 151 observations classified as 1 and 22 as 0. \n",
    "        -Applying different cut-off, say 0.5, provides for different output for the classes. \n",
    "-Note that class predictions are greatly influenced by the proportion of events in the sample. \n",
    "    -For example, in samples where there is a very small number of events the model fit may never give estimated probabilities greater than 0.4 in which case we couldn't predict class 1.\n",
    "-Confusion matrix\n",
    "    -Once we have determined class predictions we can describe the performance of the model with the confusion matrix by comparing the number of predicted vs actual classes. \n",
    "    -Confusion matrix provides information on how many observations the model classified correctly and incorrectly.\n",
    "        -The correct classifications are TN (true negatives) where all actual nonevents are predicted as nonevents and TP (true positives) where all actual events are predicted as events.\n",
    "        -FP or false positives denote incorrectly predicting nonevent as an event. Similarly, FN or false negatives incorrectly predicted events as nonevents. \n",
    "        -Like to find a balance between FPs and FNs by understanding the effect of different cutoffs and the costs associated with each error type.\n",
    "    -In Python we can use pandas crosstab() function which takes in the actual and predicted classes. \n",
    "        -We can also add rownames and columnnames as well as totals making the table more informative and easier to read. \n",
    "        -Ex: notice the imbalance between true negatives and true positives, where 104 true positives drive the overall accuracy of the model. \n",
    "        -Remember to ask yourself whether the cut-off value is optimal, i.e. does it logically apply to my problem since changing the cutoff value will change the structure of the predicted class and hence the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928637c8-0975-428f-9a0a-9c8389fbab0a",
   "metadata": {},
   "source": [
    "Exercise 10: Visualize model fit using regplot()\n",
    "After having fitted and analyzed the model we can visualize it by plotting the observation points and the fitted logistic regression.\n",
    "Using the plot you can visually understand the relationship of the explanatory variable and the response for the range of values of the explanatory variable.\n",
    "We can use the regplot() function from the seaborn module for this. The regplot() function takes an argument logistic, which allows you to specify whether you wish to estimate the logistic regression model for the given data using True or False values. This will also produce the plot of the fit.\n",
    "Recall that the model that you fitted previously:\n",
    "The dataset wells is already loaded in your workspace.\n",
    "\n",
    "Using the data wells to plot arsenic on the x-axis and switch on the y-axis.\n",
    "Apply y_jitter of 0.03 to spread the values of the response for easier visualization.\n",
    "Use True for argument logistic for the plot to overlay the logistic function on the given data and set confidence intervals argument ci to None which will not display confidence interval, but it will speed up the computation.\n",
    "Display the plot using the plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d081b-2ecf-44e2-9dc5-490a9880a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distance and switch and add overlay with the logistic fit\n",
    "sns.regplot(x = 'arsenic', y = 'switch', \n",
    "            y_jitter = 0.03,\n",
    "            data = wells, \n",
    "            logistic = True,\n",
    "            ci = None)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c66a00-4e74-4b6a-b0da-f8b2e2598113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions for the test sample wells_test and save as prediction\n",
    "prediction = wells_fit.predict(exog = wells_test)\n",
    "\n",
    "# Add prediction to the existing data frame wells_test and assign column name prediction\n",
    "wells_test['prediction'] = prediction\n",
    "\n",
    "# Examine the first 5 computed predictions\n",
    "print(wells_test[['switch', 'arsenic', 'prediction']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95265f70-e189-44f7-9156-72859197d0bd",
   "metadata": {},
   "source": [
    "     switch  arsenic  prediction\n",
    "    0       0     3.12    0.706278\n",
    "    1       0     1.69    0.583027\n",
    "    2       0     1.20    0.537289\n",
    "    3       0     1.00    0.518393\n",
    "    4       0     1.38    0.554206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f30145-17d4-454a-ad54-ccfd51df984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff\n",
    "cutoff = 0.5\n",
    "\n",
    "# Compute class predictions: y_prediction\n",
    "y_prediction = np.where(prediction > cutoff, 1, 0)\n",
    "\n",
    "# Assign actual class labels from the test sample to y_actual\n",
    "y_actual = wells_test['switch']\n",
    "\n",
    "# Compute and print confusion matrix using crosstab function\n",
    "conf_mat = pd.crosstab(y_actual, y_prediction, \n",
    "                       rownames=['Actual'], \n",
    "                       colnames=['Predicted'], \n",
    "                       margins = True)\n",
    "                      \n",
    "# Print the confusion matrix\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a195445-9090-452d-aca3-c7aa2020debf",
   "metadata": {},
   "source": [
    "## Count data and Poisson distribution\n",
    "-Concerned about the occurrence of an event, count the number of occurrences in a specified unit of time, distance, area or volume. \n",
    "    -Ex: number of goals in a soccer match, number of earthquakes in a certain region, number of crab satellite in the nest\n",
    "-Such measurements would constitute Poisson random variable if events occur independently and randomly, meaning the probability that an event occurs in a given unit of time does not change through time. \n",
    "    -Then we say that the random variable follows a Poisson distribution with parameter lambda, which describes both the mean and the variance. \n",
    "    -P(y) = ((lambda^y)*e^-lambda)/!y\n",
    "    \n",
    "-The events are always positive, discrete, not continuous and can range from zero to infinity since counts cannot be negative. \n",
    "    -Hence count data have a lower bound at zero but no upper bound. \n",
    "    -The normal distribution has no bounds. \n",
    "    -Counts can have many zero observations and be right-skewed, which add to the reasons why we wouldn't use the linear model to model count data\n",
    "-How the Poisson distribution changes as we vary the parameter lambda. \n",
    "    - Notice that when lambda is 1 the distribution is highly skewed, but as we increase lambda the distribution spreads and becomes more symmetric.\n",
    "-Visualizing the response\n",
    "    -plot your response data to visually check the distribution shape using the seaborn library and its distplot function, sns.distplot('y')\n",
    "\n",
    "## Poisson regression\n",
    "-Starting with the response y, which is count, we assume they are Poisson random variables y ~ Possion(lambda))\n",
    "    -Note that tilde means distributed as. \n",
    "    -We want to model the expected value of y, i.e. lambda. E(y) = lambda\n",
    "    -Recall that y has a constraint that it can only be positive where lambda will also be positive. \n",
    "        -To remove this constraint we take the logarithm where the log of lambda then takes values from minus infinity to infinity. log(lambda) = B0 + B1x1\n",
    "     -This defines the Poisson regression model, which is a linear combination of the parameters.\n",
    "-The explanatory variable x can be a combination of continuous and categorical variables. \n",
    "    -If all the explanatory variables are categorical then in the literature the model is referred to as the log-linear model.\n",
    "-Can fit a GLM with Poisson using the already familiar glm function from the statsmodels library. However, for count data we need to use the Poisson distribution for the family argument. The default link function is the logarithm. \n",
    "    -The formula and data arguments are the same as in logistic and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3370e-9f6b-434a-a905-03d829acc55f",
   "metadata": {},
   "source": [
    "Exercise 10: Visualize the response\n",
    "In this exercise, you will examine the response variable visually to assess the parameter value, spread of the distribution or its skewness.\n",
    "You will use the crab dataset which you used in previous chapter exercises, but now you will analyze the number of satellite crabs sat, instead of whether there is at least one, near the nesting place.\n",
    "The crab dataset has been preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f63dc-0439-4e5d-914d-b4688fa0ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot sat variable\n",
    "sns.distplot(crab['sat'])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f229b0-7bb4-44b8-b9db-c708878b92ae",
   "metadata": {},
   "source": [
    "Outcome:\n",
    "Visualizing the response variable there is apparent skewness of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f196c-fd53-4411-8d0e-f8d683ae7a3b",
   "metadata": {},
   "source": [
    "Exercise 11: Fitting a Poisson regression\n",
    "Continuing with the crab dataset you will fit your first Poisson regression model in this exercise.\n",
    "The crab dataset has been preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e79ef-53ff-44fb-bb86-bac5ad75010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit Poisson regression of sat by weight\n",
    "model = glm('sat ~ weight', data = crab, family = sm.families.Poisson()).fit()\n",
    "\n",
    "# Display model results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ab1ad-e9c6-4c4d-87d3-56a6cddc583c",
   "metadata": {},
   "source": [
    "## Interpreting model fit\n",
    "-Parameter estimation\n",
    "    -Similarly as in logistic regression, the maximum likelihood estimation, is used to obtain the values of betas, the parameters, which maximize the log-likelihood function. \n",
    "    -As a result, the general principles and inference procedures carry over to Poisson regression analysis, like confidence intervals and hypothesis tests.\n",
    "-The response function: we defined the Poisson regression as follows with the log link function providing for the linear combination in the parameters log(lambda) = B0 + B1x1 \n",
    "    -Since parameters are on the log scale, we need to exponentiate to obtain the response function in terms of lambda. \n",
    "    -Note that the log link exponentiates the linear predictors but does not transform the response variable. Having this form we can interpret the effect of beta on the response.\n",
    "    -Note that the effect of x on lambda is multiplicative. lambda=exp(B0 + B1x1)  ---> lambda=exp(B0) x exp(B1x1)\n",
    "-Interpretation of parameters\n",
    "    -B1is the expected difference in y on a logarithmic scale for a 1-unit increase in x\n",
    "    -exp(B1) is the expected multiplicative effect on the mean lambda for a 1-unit increase in x.\n",
    "-To interpret the effect of the coefficient on the mean of the response we consider 3 cases. \n",
    "    -B1>zero then the exp(B1) is greater than 1. Hence lambda is exp(B1) times larger than when x is zero. \n",
    "    -B1<zero then the exp(B1) is less than one and lambda is exp(B1) time smaller than when x is zero. \n",
    "    -B1=zero then the exp(B1) is 1 leading to no effect on the response and hence the explanatory variable and the response are not related. \n",
    "        -Note that we are comparing the effect based on 1 given the inherent exponential scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac1e9f-0db0-400a-96d3-45e6ff4ea3a6",
   "metadata": {},
   "source": [
    "Exercise 12: Estimate parameter lambda\n",
    "In this exercise, you will use this formulation with the horseshoe crab data to compute the estimate of the mean for the female crab width.\n",
    "Dataset crab is preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658948e-d24c-4707-a75f-ff7d2320e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit Poisson regression of sat by width\n",
    "model = glm('sat ~ width', data = crab, family = sm.families.Poisson()).fit()\n",
    "\n",
    "# Display model results\n",
    "print(model.summary())\n",
    "\n",
    "# Compute average crab width\n",
    "mean_width = np.mean(crab['width'])\n",
    "\n",
    "# Print the compute mean\n",
    "print('Average width: ', round(mean_width, 3))\n",
    "\n",
    "# Extract coefficients\n",
    "intercept, slope = model.params\n",
    "\n",
    "# Compute the estimated mean of y (lambda) at the average width\n",
    "est_lambda = np.exp(intercept) * np.exp(slope * mean_width)\n",
    "\n",
    "# Print estimated mean of y\n",
    "print('Estimated mean of y at average width: ', round(est_lambda, 3))\n",
    "\n",
    "# Extract coefficients\n",
    "intercept, slope = model.params\n",
    "\n",
    "# Compute and print the multiplicative effect\n",
    "print(np.exp(slope))\n",
    "\n",
    "# Compute confidence intervals for the coefficients\n",
    "model_ci = model.conf_int()\n",
    "\n",
    "# Compute and print the confidence intervals for the multiplicative effect on the mean\n",
    "print(np.exp(model_ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25affa-c956-40bd-a7ee-4357ec0fd812",
   "metadata": {},
   "source": [
    "Outcome:\n",
    "The Poisson regression model states that at the mean value of female crab width of 26.3 the expected mean number of satellite crabs present is 2.74.\n",
    "To conclude a 1-unit increase in female crab width the number of satellite crabs will increase, which will be multiplied by 1.18.\n",
    "The multiplicative effect on the mean response for a 1-unit increase in width is at least 1.13 and at most 1.22."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e318c-7063-404f-a989-41ce820950ee",
   "metadata": {},
   "source": [
    "## The Problem of Overdispersion\n",
    "-you learned about Poisson distribution and its parameter lambda, which represents both the mean and the variance. \n",
    "    -Using this assumption we have fitted Poisson regression models: violating this assumption and what measures you can take to remedy the problem.\n",
    "-If the variance is not equal to the mean? Overdispersion:\n",
    "    -check mean/variance of repsonse variable\n",
    "    -Note that overdispersion is not an issue in linear models assuming normally distributed response variable since there is a separate parameter, which describes the variability in the data. \n",
    "    -As a comparison to overdispersion with the variance larger than the mean, there is also an effect called underdispersion, which occurs when the variance is less than the mean, but this is rare in actual data. \n",
    "    -Overdispersion usually results in incorrect small standard errors and p-values of the model coefficients. \\\n",
    "        -Hence, we have to be careful when interpreting model results.\n",
    "-How to check for overdispersion?\n",
    "    -We can estimate for the presence of overdispersion using the Pearson statistic and the degrees of freedom of the fitted model. \n",
    "    -In the model summary, we are provided with the information on the Pearson chi-squared statistic and the degrees of freedom of the residuals.\n",
    "        -Compute estimated overdispersion\n",
    "        -To estimate for overdispersion we check the ratio of the Pearson statistic with the reported degrees of freedom of the residuals. \n",
    "        -Obtain the value of the Pearson statistic with the Pearson_chi_squared() function of the model fit, and similarly, the function df_resid() provides the degrees of freedom of the residuals. \n",
    "        -The decision is made based on whether the ratio is greater than 1. \n",
    "            -if the ratio is close to 1 then it is assumed the data is drawn from the Poisson distribution\n",
    "            -if it is smaller than 1 then it implies underdispersion\n",
    "            -if greater than one it implied overdispersion. \n",
    "        -Note that this is an approximation and there is no fixed threshold for an affirmative statistical intervention.\n",
    "-Negative Binomial Regression\n",
    "    -To account for overdispersion in the data we can fit a negative Binomial regression model, where the negative Binomial is a generalization of the Poisson distribution. \n",
    "    -The negative Binomial uses additional parameter alpha, the dispersion parameter, which specifies the level that the distribution's variance exceeds its mean. \n",
    "        E(y) = lambda ---> Var(y) = lambda + alpha(lambda^2)\n",
    "    -Notice that as alpha goes to zero the variance is equal to the mean. .NegativeBinomial(alpha = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c429ee-59af-4aa0-8c6f-7bcf6c0ce9d2",
   "metadata": {},
   "source": [
    "Exercise 13: Is the mean equal to the variance?\n",
    "Under the Poisson model one of the assumptions was that the mean should be the same as the variance. As you learned in the lecture, if this assumption is violated then there is overdispersion. \n",
    "Without adjusting for overdispersion you would wrongly interpret standard errors of the given model.\n",
    "In this exercise you will first compute the mean and the variance of the number of satellites for the female crabs.\n",
    "The crab dataset is loaded in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b8d09-05c3-4800-9de9-aebd22793b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print sample mean of the number of satellites: sat_mean\n",
    "sat_mean = np.mean(crab.sat)\n",
    "\n",
    "print('Sample mean:', round(sat_mean, 3))\n",
    "\n",
    "# Compute and print sample variance of the number of satellites: sat_var\n",
    "sat_var = np.var(crab.sat)\n",
    "print('Sample variance:', round(sat_var, 3))\n",
    "\n",
    "# Compute ratio of variance to mean\n",
    "print('Ratio:', round(sat_var/sat_mean, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e848cbd-ea48-4c54-8bde-4a25c598a7ca",
   "metadata": {},
   "source": [
    "Notice that the variance is 3.37 times the mean. This gives an indication that Poisson GLM will not provide the most accurate fit to the data. Let's do another check before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7bbcd-82e6-4517-9129-bd5e03b33907",
   "metadata": {},
   "source": [
    "Exercise 14: Computing expected number of counts\n",
    "In this exercise you will practice another analysis for overdispersion by using the already computed mean and calculating the expected number of counts per certain value of counts, for example zero counts. \n",
    "In other words, what count of zero satellites should we expect in the sample given the computed sample mean.\n",
    "Recall figure from the crab dataset where you can notice a large number of zero counts.\n",
    "Recall that to compute the expected number of counts given the parameter you can use the defined Poisson distribution, given by\n",
    "The crab dataset and the computed mean sat_mean is preloaded in the workspace.\n",
    "\n",
    "Using computed mean sat_mean and the zero counts  compute the expected number of zero counts. Use math factorial().\n",
    "Compute the number of observations with zero counts in the sat variable using the sum() and the total number of observations in the sample using the len() functions.\n",
    "Print the ratio of actual zero count observations and total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3f844-7b7a-44b0-8313-dd183e75a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected number of zero counts\n",
    "exp_zero_cnt = ((sat_mean**0)*np.exp(-sat_mean))/math.factorial(0)\n",
    "\n",
    "# Print exp_zero_counts\n",
    "print('Expected zero counts given mean of ', round(sat_mean,3), \n",
    "      'is ', round(exp_zero_cnt,3)*100)\n",
    "\n",
    "# Number of zero counts in sat variable\n",
    "actual_zero_ant = sum(crab['sat']  == 0)\n",
    "\n",
    "# Number of observations in crab dataset\n",
    "num_obs = len(crab)\n",
    "\n",
    "# Print the percentage of zero count observations in the sample\n",
    "print('Actual zero counts in the sample: ', round(actual_zero_ant / num_obs,3)*100)\n",
    "\n",
    "# Compute and print the overdispersion approximation\n",
    "print(crab_pois.pearson_chi2 / crab_pois.df_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3696652-906f-4ec5-bc3d-bf1e1d3b8277",
   "metadata": {},
   "source": [
    "Expected zero counts given mean of  2.919 is  5.4\n",
    "Actual zero counts in the sample:  35.8\n",
    "    \n",
    "Outcome:\n",
    "Notice that given the mean parametar there should be 5.4% observations with zero count, but in the crab sample there are 35.8% observations with zero count, indicating the presence of overdispersion.\n",
    "There is overdispersion present since the pearson ratio is greater than 1, meaning that the coefficient estimates should not be interpreted directly, you will solve this problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b096f9-2cd2-4765-bf14-ad3b765b9537",
   "metadata": {},
   "source": [
    "Exercise 15: Fitting negative binomial\n",
    "The negative binomial allows for the variance to exceed the mean, which is what you have measured in the previous exercise in your data crab. In this exercise you will recall the previous fit of the Poisson regression using the log link function and additionally fit negative binomial model also using the log link function.\n",
    "You will analyze and see how the statistical measures were changed.\n",
    "The model crab_pois and crab is loaded in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4fc0a-22ca-4b1d-ac0b-f830690912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formula for the model fit\n",
    "formula = 'sat ~ width'\n",
    "\n",
    "# Fit the GLM negative binomial model using log link function\n",
    "crab_NB = smf.glm(formula = formula, data = crab, \n",
    "\t\t\t\t  family = sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Print Poisson model's summary\n",
    "print(crab_pois.summary())\n",
    "\n",
    "# Print the negative binomial model's summary\n",
    "print(crab_NB.summary())\n",
    "\n",
    "# Compute confidence intervals for crab_Pois model\n",
    "print('Confidence intervals for the Poisson model')\n",
    "print(crab_pois.conf_int())\n",
    "\n",
    "# Compute confidence intervals for crab_NB model\n",
    "print('Confidence intervals for the Negative Binomial model')\n",
    "print(crab_NB.conf_int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04e6e6-7822-4006-b973-dbd57de7437a",
   "metadata": {},
   "source": [
    "                     Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                    sat   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      171\n",
    "    Model Family:                 Poisson   Df Model:                            1\n",
    "    Link Function:                    log   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -461.59\n",
    "    Date:                Wed, 08 Jun 2022   Deviance:                       567.88\n",
    "    Time:                        20:31:00   Pearson chi2:                     544.\n",
    "    No. Iterations:                     5   Covariance Type:             nonrobust\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept     -3.3048      0.542     -6.095      0.000      -4.368      -2.242\n",
    "    width          0.1640      0.020      8.216      0.000       0.125       0.203\n",
    "    ==============================================================================\n",
    "                     Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                    sat   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      171\n",
    "    Model Family:        NegativeBinomial   Df Model:                            1\n",
    "    Link Function:                    log   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -375.80\n",
    "    Date:                Wed, 08 Jun 2022   Deviance:                       206.41\n",
    "    Time:                        20:31:00   Pearson chi2:                     155.\n",
    "    No. Iterations:                     6   Covariance Type:             nonrobust\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept     -4.0323      1.129     -3.572      0.000      -6.245      -1.820\n",
    "    width          0.1913      0.042      4.509      0.000       0.108       0.274\n",
    "    ==============================================================================\n",
    "    \n",
    "Well done! Notice how standard error increased to 0.042, reflecting overdispersion which was not captured with the Poisson model.\n",
    "\n",
    "Confidence intervals for the Poisson model\n",
    "                      0         1\n",
    "    Intercept -4.367531 -2.241983\n",
    "    width      0.124914  0.203176\n",
    "    Confidence intervals for the Negative Binomial model\n",
    "                      0         1\n",
    "    Intercept -6.244509 -1.820000\n",
    "    width      0.108155  0.274472\n",
    "    \n",
    "Notice how the confidence intervals are wider for the negative Binomial model compared to quite narrow confidence intervals for the Poisson model since it did not account for overdispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e31287-3b23-4626-a6c4-d9f7715bb99f",
   "metadata": {},
   "source": [
    "Exercise 16: Plotting data and linear model fit\n",
    "In the previous exercises you have practiced how to fit and interpret the Poisson regression model. In this exercise you will visually analyze the crab data and then the model fit.\n",
    "\n",
    "First, you will plot a linear fit to the data, which later on you will use to compare to Poisson regression fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a2b11-c709-4f36-a9cc-c7880c9299e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data points and linear model fit\n",
    "sns.regplot('width', 'sat', data = crab,\n",
    "            y_jitter = 0.3,\n",
    "            fit_reg =True,\n",
    "            line_kws = {'color':'green', \n",
    "                        'label':'LM fit'})\n",
    "\n",
    "# Print plot\n",
    "plt.show()\n",
    "\n",
    "# Add fitted values to the fit_values column of crab dataframe\n",
    "crab['fit_values'] = model.fittedvalues\n",
    "\n",
    "# Plot data points\n",
    "sns.regplot('width', 'sat', data = crab,\n",
    "            y_jitter = 0.3,\n",
    "            fit_reg = True, \n",
    "            line_kws = {'color':'green', \n",
    "                        'label':'LM fit'})\n",
    "\n",
    "# Poisson regression fitted values\n",
    "sns.scatterplot('width','fit_values', data = crab,\n",
    "           color = 'red', label = 'Poisson')\n",
    "\n",
    "# Print plot          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d4e10-9b36-469e-979e-c170fec84bff",
   "metadata": {},
   "source": [
    "c\n",
    "Similary as for the model with weight variable the linear and Poisson fits are close in the mid range of width values, but diverge on smaller and larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d516ba1-9604-4ea1-abeb-63fcffa6144b",
   "metadata": {},
   "source": [
    "## Multiple Regression\n",
    "-Presence of multicollinearity\n",
    "    -Variance inflation factor (VIF): The most widely used diagnostic for multicollinearity is the variance inflation factor of each explanatory variable. \n",
    "        -It describes how inflated the variance of the coefficient is compared to what it would be if the variables were not correlated with any other variable in the model. \n",
    "        -A general threshold can be set at a value of 2.5. \n",
    "        -In Python, we can compute VIF directly using the variance inflation factor function from statsmodels library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959b1dc-49f4-4e19-97a8-338c19bfaff8",
   "metadata": {},
   "source": [
    "Exercise 17: Fit a multivariable logistic regression\n",
    "Using the knowledge gained in the video you will revisit the crab dataset to fit a multivariate logistic regression model. In chapter 2 you have fitted a logistic regression with width as explanatory variable. In this exercise you will analyze the effects of adding color as additional variable.\n",
    "The color variable has a natural ordering from medium light, medium, medium dark and dark. As such color is an ordinal variable which in this example you will treat as a quantitative variable.\n",
    "The crab dataset is preloaded in the workspace. Also note that the only difference in the code from the univariate case is in the formula argument, where now you will add structure to incorporate the new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19446fee-d054-4b23-8a3e-a29c82154629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Define model formula\n",
    "formula = 'y ~ width + color'\n",
    "\n",
    "# Fit GLM\n",
    "model = glm(formula, data = crab, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf1034-c368-4e80-961c-494b00671407",
   "metadata": {},
   "source": [
    "Outcome:\n",
    "You fitted your first multivariable logistic regression. From model summary note that for each one-level increase in color of the female crab, the estimated odds multiply by exp(-0.509)=0.6, i.e. the odds for dark crabs are 60% than those for medium crabs.\n",
    "\n",
    "Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      170\n",
    "    Model Family:                Binomial   Df Model:                            2\n",
    "    Link Function:                  logit   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -94.561\n",
    "    Date:                Fri, 10 Jun 2022   Deviance:                       189.12\n",
    "    Time:                        14:53:30   Pearson chi2:                     170.\n",
    "    No. Iterations:                     5   Covariance Type:             nonrobust\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept    -10.0708      2.807     -3.588      0.000     -15.572      -4.569\n",
    "    width          0.4583      0.104      4.406      0.000       0.254       0.662\n",
    "    color         -0.5090      0.224     -2.276      0.023      -0.947      -0.071\n",
    "    =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20609ee0-6a51-4bf2-8aa4-991199ed5391",
   "metadata": {},
   "source": [
    "Exercise 18: The effect of multicollinearity\n",
    "Using the crab dataset you will analyze the effects of multicollinearity. Recall that multicollinearity can have the following effects:\n",
    "\n",
    "Coefficient is not significant, but variable is highly correlated with .\n",
    "Adding/removing a variable significantly changes coefficients.\n",
    "Not logical sign of the coefficient.\n",
    "Variables have high pairwise correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03036d7-e800-4ce1-a756-3841bb712d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "\n",
    "# Define model formula\n",
    "formula = 'y ~ weight + width'\n",
    "\n",
    "# Fit GLM\n",
    "model = glm(formula, data = crab, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e79f3-1a15-4288-9796-886c648f7ecd",
   "metadata": {},
   "source": [
    "Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      170\n",
    "    Model Family:                Binomial   Df Model:                            2\n",
    "    Link Function:                  logit   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -96.446\n",
    "    Date:                Fri, 10 Jun 2022   Deviance:                       192.89\n",
    "    Time:                        14:55:50   Pearson chi2:                     167.\n",
    "    No. Iterations:                     5   Covariance Type:             nonrobust\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept     -9.3547      3.528     -2.652      0.008     -16.270      -2.440\n",
    "    weight         0.8338      0.672      1.241      0.214      -0.483       2.150\n",
    "    width          0.3068      0.182      1.686      0.092      -0.050       0.663\n",
    "    ==============================================================================\n",
    "Notice that the neither weight nor width are statistically significant. Recall that when we fitted univariate logistic regressions for each variable, both variables where statistically significant. There is evident presence of multicollinearity! Let's measure it in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bca06-c253-41f6-8430-b055781c5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Get variables for which to compute VIF and add intercept term\n",
    "X = crab[['weight', 'width', 'color']]\n",
    "X['Intercept'] = 1\n",
    "\n",
    "# Compute and view VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# View results using print\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068be6e-8df8-4740-b8d0-6d3e4ee747e4",
   "metadata": {},
   "source": [
    "variables         VIF\n",
    "    0     weight    4.691018\n",
    "    1      width    4.726378\n",
    "    2      color    1.076594\n",
    "    3  intercept  414.163343\n",
    "With VIF well above 2.5 for weight and width means that there is multicollinearity present in the model and you can not use both variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4740d1e-e317-4814-9267-e03d0d324e36",
   "metadata": {},
   "source": [
    "## Deviance\n",
    "-How good model?: To answer this we consider a goodness-of-fit measure called deviance statistic, which tests the null hypothesis that the fitted model is correct.\n",
    "    -With the goodness of fit we are measuring whether our model is correctly specified and if we add more complexity would it be better. Complexity in this sense means adding more variables, non-linear or interaction terms. \n",
    "    -Deviance is measured in terms of log-likelihood, where formally it is defined as negative two times the log likelihood of the model fit. D = -2LL(Beta) \n",
    "    -It represents a measure of an error where lower deviance means better model fit. \n",
    "    -For benchmark, we use the null deviance, i.e. the deviance from the model with only the intercept term. \n",
    "    -The idea is that as we add additional variables to the model the deviance would decrease therefore providing for a better fit. \n",
    "    -Generally, it is assumed that if we were to add a variable with random noise the deviance would decrease by one, so if we add p predictors to the model the deviance should decrease by more than p.\n",
    "    \n",
    "## Model complexity\n",
    "-It is important to note that increasing the number of variables in the model and reducing the deviance may not provide a clear cut path towards a better model fit. \n",
    "    -Say we have two models with likelihoods L1 and L2 where the likelihood of model 2 is lower. \n",
    "    -We might say that L2 provides the \"better\" fit, however, we also need to take into consideration the model complexity or the number of parameters to be estimated in model with L2 likelihood compared to model 1. \n",
    "    -It can happen that when applying both models to new data model one will produce a better fit than model 2, providing that model 2 is overfitting the training dataset and actually has worse fit on new data. \n",
    "    -In such situations, we say that model 2 does not generalize well on unseen data. \n",
    "    -If this occurs we would need to reduce model complexity to reduce overfitting and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbed650-5ad8-499b-b368-95075ff46e46",
   "metadata": {},
   "source": [
    "Exercise 19: Checking model fit\n",
    "In the video you analyzed the example of an improvement in the model fit by adding additional variable on the wells data. Continuing with this data set you will see how further increase in model complexity effects deviance and model fit.\n",
    "The dataset wells have been preloaded in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17829fc-f9b9-41e4-bf71-26b938f1378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Define model formula\n",
    "formula = 'switch ~ distance100 + arsenic'\n",
    "\n",
    "# Fit GLM\n",
    "model_dist_ars = glm(formula, data = wells, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Compare deviance of null and residual model\n",
    "diff_deviance = model_dist_ars.null_deviance - model_dist_ars.deviance\n",
    "\n",
    "# Print the computed difference in deviance\n",
    "print(diff_deviance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40c0f2-f9d4-426a-b0ab-9d182ef0676e",
   "metadata": {},
   "source": [
    "Having both distance100 and arsenic in the model reduces deviance by 187 compared to the intercept only model. But what is the actual impact of additional variable arsenic? Let's see in the next exericise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51276a-83b5-49c3-9626-f1946f119927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference in adding distance100 variable\n",
    "diff_deviance_distance = model_dist.null_deviance - model_dist.deviance\n",
    "\n",
    "# Print the computed difference in deviance\n",
    "print('Adding distance100 to the null model reduces deviance by: ', \n",
    "      round(diff_deviance_distance,3))\n",
    "\n",
    "# Compute the difference in adding arsenic variable\n",
    "diff_deviance_arsenic = model_dist.deviance - model_dist_ars.deviance\n",
    "\n",
    "# Print the computed difference in deviance\n",
    "print('Adding arsenic to the distance model reduced deviance further by: ', \n",
    "      round(diff_deviance_arsenic,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b58c8b-2088-4da4-a6c6-9e4f52108674",
   "metadata": {},
   "source": [
    "Adding distance100 to the null model reduces deviance by:  41.861\n",
    "Adding arsenic to the distance model reduced deviance further by:  145.57\n",
    "\n",
    "Outcome:\n",
    "Adding distance100 to the null model reduces deviance by 41.9 and with an addition of arsenic the deviance further reduces by 145. Having such large reduction than expected reduction by 1 we can conclude that the multivariate model has improved the model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf48994-489f-41f6-8303-7eaf2149b709",
   "metadata": {},
   "source": [
    "Exercise 20: Deviance and linear transformation\n",
    "As you have seen in previous exercises the deviance decreased as you added a variable that improves the model fit. In this exercise you will consider the well switch data example and the model you fitted with distance variable, but you will assess what happens when there is a linear transformation of the variable.\n",
    "\n",
    "Note that the variable distance100 is the original variable distance divided by 100 to make for more meaningful representation and interpretation of the results. You can inspect the data with wells.head() to view the first 5 rows of data.\n",
    "\n",
    "The wells dataset and the model 'swicth ~ distance100' has been preloaded as model_dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed009825-d5e0-4063-ab86-318df1b5d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit logistic regression model as save as model_dist_1\n",
    "model_dist_1 = glm('switch ~ distance', data = wells, family =sm.families.Binomial()).fit()\n",
    "\n",
    "# Check the difference in deviance of model_dist_1 and model_dist\n",
    "print('Difference in deviance is: ', round(model_dist_1.deviance - model_dist.deviance,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1011c2f-13ee-4602-9606-33dff93e21e6",
   "metadata": {},
   "source": [
    "Difference in deviance is:  0.0\n",
    "Great! Note that linear transformations do not change the model error and hence the deviance remains the same. The reason being since linear transformation does not add new data information to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee4ec7-4e95-4aa2-aff9-da26af2e2dd4",
   "metadata": {},
   "source": [
    "## View formmula 'y ~ x1 + x2' as Model matrix\n",
    "-To see the model matrix we use dmatrix function from the patsy package with only the RHS of the formula. \n",
    "-While we are predominantly concerned with the formula specification viewing model matrix especially with more complex model structure is always advisable practice.\n",
    "\n",
    "## Centering and standardization\n",
    "-Centering or standardizing variables can also be done directly in the formula argument. \n",
    "-Centering subtracts the mean\n",
    "-standardizing subtracts the mean and divides by the sample standard deviation. \n",
    "-These are stateful transforms as they remember the state of the original data which then can be further used to apply to new data.\n",
    "\n",
    "## Arithmetic operations\n",
    "-Direct arithmetic transformations are also possible but we need to wrap them. \n",
    "-To model the sum of x1 and x2 instead of individual values we need to enclose them in the I function. \n",
    "-From the output, we can see that we obtain the intercept and only one explanatory variable. \n",
    "-Note however that if the variables are in the standard list (not arrays) then it will perform concatenation and not element-wise addition\n",
    "\n",
    "\n",
    "## Patsy coding\n",
    "The strings and booleans are automatically codded, where for other categorical data we use the C function. By default, the 1st group is used as a reference, which can be changed using Treatment or levels argument.\n",
    "The C() function\n",
    "The color variable in the crab dataset is defined as an integer, so the model matrix would take it as such. However, using the value counts function we see there are 4 levels.\n",
    "The C() function\n",
    "Applying the C function codes the color variable. Since color takes 4 different groups, it is represented with 3 columns and a reference group. The mean behavior of the reference group is given by the intercept. The 3 columns have values of 0 or Changing the reference group\n",
    "Using treatment inside the C function and declaring the level changes the reference group.\n",
    "Changing the reference group\n",
    "Similarly you can use levels inside the C argument for different coding of the reference group.\n",
    "Multiple intercepts\n",
    "To estimate the intercept for each group we would add negative one in the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7cf7a-2f29-4f82-8495-17254df1c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function dmatrix()\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct model matrix with arsenic\n",
    "model_matrix = dmatrix('arsenic', data = wells, return_type = 'dataframe')\n",
    "print(model_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b48999-aec1-49c0-9b6f-4810365d8d75",
   "metadata": {},
   "source": [
    "Intercept  arsenic\n",
    "    0        1.0     2.36\n",
    "    1        1.0     0.71\n",
    "    2        1.0     2.07\n",
    "    3        1.0     1.15\n",
    "    4        1.0     1.10\n",
    "\n",
    "<script.py> output:\n",
    "       Intercept  arsenic  distance100\n",
    "    0        1.0     2.36      0.16826\n",
    "    1        1.0     0.71      0.47322\n",
    "    2        1.0     2.07      0.20967\n",
    "    3        1.0     1.15      0.21486\n",
    "    4        1.0     1.10      0.40874\n",
    "Outcome:\n",
    "Nice job! Notice how dmatrix() silently includes an intercept for each model matrix without you specifying it. Analyzing the output from dmatrix()you can be sure that the inputs are correctly structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cf79f-d07a-4449-8604-ec44fc034964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function dmatrix\n",
    "import numpy as np\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct model matrix for arsenic with log transformation\n",
    "dmatrix('np.log(arsenic)', data = wells,\n",
    "       return_type = 'dataframe').head()\n",
    "\n",
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "import numpy as np\n",
    "\n",
    "# Define model formula\n",
    "formula = 'switch ~ np.log(arsenic)'\n",
    "\n",
    "# Fit GLM\n",
    "model_log_ars = glm(formula, data = wells, \n",
    "                     family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model_log_ars.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769370a3-5161-4b31-98bd-72af8875f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function dmatrix\n",
    "from patsy import dmatrix\n",
    "\n",
    "\n",
    "# Construct and print model matrix for color as categorical variable\n",
    "print(dmatrix('C(color)', data = crab,\n",
    "     \t   return_type = 'dataframe').head())\n",
    "\n",
    "# Import function dmatrix\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct and print the model matrix for color with reference group 3\n",
    "print(dmatrix('C(color, Treatment(4))', data = crab,\n",
    "     \t   return_type = 'dataframe').head())\n",
    "\t\t   \n",
    "    \n",
    "    \n",
    "# Construct model matrix\n",
    "model_matrix = dmatrix('C(color, Treatment(4))' , data = crab, \n",
    "                       return_type = 'dataframe')\n",
    "\n",
    "# Print first 5 rows of model matrix dataframe\n",
    "print(model_matrix.head())\n",
    "\n",
    "# Fit and print the results of a glm model with the above model matrix configuration\n",
    "model = glm('y ~ C(color, Treatment(4))', data = crab, \n",
    "            family = sm.families.Binomial()).fit()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Construct model matrix\n",
    "model_matrix = dmatrix('C(color, Treatment(4)) + width' , data = crab, \n",
    "                       return_type = 'dataframe')\n",
    "\n",
    "# Print first 5 rows of model matrix\n",
    "print(model_matrix.head())\n",
    "\n",
    "# Fit and print the results of a glm model with the above model matrix configuration\n",
    "model = glm('y ~ C(color, Treatment(4)) + width', data = crab, \n",
    "            family = sm.families.Binomial()).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38dbfa2-c914-4841-9adf-c47cee658e30",
   "metadata": {},
   "source": [
    "Intercept  C(color, Treatment(4))[T.1]  C(color, Treatment(4))[T.2]  C(color, Treatment(4))[T.3]\n",
    "    0        1.0                          0.0                          1.0                          0.0\n",
    "    1        1.0                          0.0                          0.0                          1.0\n",
    "    2        1.0                          1.0                          0.0                          0.0\n",
    "    3        1.0                          0.0                          0.0                          1.0\n",
    "    4        1.0                          0.0                          0.0                          1.0\n",
    "                     Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      169\n",
    "    Model Family:                Binomial   Df Model:                            3\n",
    "    Link Function:                  logit   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -106.03\n",
    "    Date:                Fri, 10 Jun 2022   Deviance:                       212.06\n",
    "    Time:                        16:14:34   Pearson chi2:                     173.\n",
    "    No. Iterations:                     4   Covariance Type:             nonrobust\n",
    "    ===============================================================================================\n",
    "                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    Intercept                      -0.7621      0.458     -1.665      0.096      -1.659       0.135\n",
    "    C(color, Treatment(4))[T.1]     1.8608      0.809      2.301      0.021       0.276       3.446\n",
    "    C(color, Treatment(4))[T.2]     1.7382      0.512      3.393      0.001       0.734       2.742\n",
    "    C(color, Treatment(4))[T.3]     1.1299      0.551      2.051      0.040       0.050       2.210\n",
    "    ===============================================================================================\n",
    "    \n",
    "The estimated odds that a crab with color_2 (medium) has a satellite nearby are 5.687 times the estimated odds that a crab with color_4 (dark) has a satellite present.\n",
    "\n",
    "\n",
    "Intercept  C(color, Treatment(4))[T.1]  C(color, Treatment(4))[T.2]  C(color, Treatment(4))[T.3]  width\n",
    "    0        1.0                          0.0                          1.0                          0.0   28.3\n",
    "    1        1.0                          0.0                          0.0                          1.0   22.5\n",
    "    2        1.0                          1.0                          0.0                          0.0   26.0\n",
    "    3        1.0                          0.0                          0.0                          1.0   24.8\n",
    "    4        1.0                          0.0                          0.0                          1.0   26.0\n",
    "                     Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                  173\n",
    "    Model:                            GLM   Df Residuals:                      168\n",
    "    Model Family:                Binomial   Df Model:                            4\n",
    "    Link Function:                  logit   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -93.729\n",
    "    Date:                Fri, 10 Jun 2022   Deviance:                       187.46\n",
    "    Time:                        16:19:45   Pearson chi2:                     169.\n",
    "    No. Iterations:                     5   Covariance Type:             nonrobust\n",
    "    ===============================================================================================\n",
    "                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    Intercept                     -12.7151      2.762     -4.604      0.000     -18.128      -7.302\n",
    "    C(color, Treatment(4))[T.1]     1.3299      0.853      1.560      0.119      -0.341       3.001\n",
    "    C(color, Treatment(4))[T.2]     1.4023      0.548      2.557      0.011       0.327       2.477\n",
    "    C(color, Treatment(4))[T.3]     1.1061      0.592      1.868      0.062      -0.054       2.267\n",
    "    width                           0.4680      0.106      4.434      0.000       0.261       0.675\n",
    "    ===============================================================================================\n",
    "    \n",
    "A one-unit increase in width has multiplicative effect of 1.5967 on the odds that the satellite is nearby for all color groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cb8fc-e2a8-4e66-8e0d-43187b322f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit GLM and print model summary\n",
    "model_int = glm('switch ~ center(distance100) + center(arsenic) + center(distance100):center(arsenic)', \n",
    "                data = wells, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# View model results\n",
    "print(model_int.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ec8ba-2cbf-4779-9759-62e6fb7d3925",
   "metadata": {},
   "source": [
    "Generalized Linear Model Regression Results                  \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                 switch   No. Observations:                 3020\n",
    "    Model:                            GLM   Df Residuals:                     3016\n",
    "    Model Family:                Binomial   Df Model:                            3\n",
    "    Link Function:                  logit   Scale:                          1.0000\n",
    "    Method:                          IRLS   Log-Likelihood:                -1963.8\n",
    "    Date:                Fri, 10 Jun 2022   Deviance:                       3927.6\n",
    "    Time:                        16:23:41   Pearson chi2:                 3.09e+03\n",
    "    No. Iterations:                     4   Covariance Type:             nonrobust\n",
    "    =======================================================================================================\n",
    "                                              coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    -------------------------------------------------------------------------------------------------------\n",
    "    Intercept                               0.3511      0.040      8.810      0.000       0.273       0.429\n",
    "    center(distance100)                    -0.8737      0.105     -8.337      0.000      -1.079      -0.668\n",
    "    center(arsenic)                         0.4695      0.042     11.159      0.000       0.387       0.552\n",
    "    center(distance100):center(arsenic)    -0.1789      0.102     -1.748      0.080      -0.379       0.022\n",
    "    =======================================================================================================\n",
    "    \n",
    "Question\n",
    "To interpret the interaction parameter we need to consider the effect of its coefficient on both arsenic and distance100 estimates. Hence, for a one-unit change in the explanatory variable the interaction coefficient is added to each coefficient for individual variable.\n",
    "\n",
    "Considering the model summary of the previously fitted model, which of the following is the correct interpretation of the model with interaction term?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "The interaction term increases the importance of distance100 as explanatory variable given one unit increase in arsenic levels.\n",
    "\n",
    "The interaction term decreases the importance of arsenic as explanatory variable given one unit increase in distance100 values.\n",
    "\n",
    "At average value of distance100 and arsenic the probability of switching from the current well is equal to 0.59. \n",
    "\n",
    "All of the above.\n",
    "\n",
    "Submit Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
